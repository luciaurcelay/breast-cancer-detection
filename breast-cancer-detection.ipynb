{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em_-pK1xHsJK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import *\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from numpy import genfromtxt\n",
        "from PIL import Image, ImageEnhance\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAtjpUquAWX-",
        "outputId": "5d407e73-78a8-4f7b-8b8f-1b7b2a1472c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RSpyMlpY_FZ"
      },
      "source": [
        "# Dataset and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LABELS = ['Normal', 'Benign', 'InSitu', 'Invasive']\n",
        "IMAGE_SIZE = (2048, 1536)\n",
        "PATCH_SIZE = 512\n",
        "PROJECT_PATH = '/content/gdrive/MyDrive/CI Final Project'\n",
        "DATASET_PATH = f'{PROJECT_PATH}/ICAR 2018 BACH Dataset'\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "vTpJa4A5Mou_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes_for_dataset = [i for i in range(400)]\n",
        "TRAIN_INDEXES, TEST_INDEXES = train_test_split(indexes_for_dataset, test_size=0.1, random_state=5)"
      ],
      "metadata": {
        "id": "oNxsTcGvWeGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __crop_img__(img, size, n):\n",
        "    h, w, c = img.shape\n",
        "    crops = []\n",
        "\n",
        "    for _ in range(n):\n",
        "        top = np.random.randint(low=0, high=h - size + 1)\n",
        "        left = np.random.randint(low=0, high=w - size + 1)\n",
        "        crop = img[top: top + size, left: left + size].copy()\n",
        "        crop = np.rot90(crop, np.random.randint(low=0, high=4))\n",
        "        if np.random.random() > 0.5:\n",
        "            crop = np.flipud(crop)\n",
        "        if np.random.random() > 0.5:\n",
        "            crop = np.fliplr(crop)\n",
        "        crops.append(crop)\n",
        "\n",
        "    crops = np.stack(crops)\n",
        "    return crops"
      ],
      "metadata": {
        "id": "2Nx22qVojf9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class ICARDataset(Dataset):\n",
        "    def __init__(self, path, rotate=False, flip=False, enhance=False, train=False):\n",
        "        super().__init__()\n",
        "\n",
        "        dataset = {name: index for index in range(len(LABELS)) for name in glob.glob(path + '/' + LABELS[index] + '/*.tif')}\n",
        "        self.path = path\n",
        "        self.dataset = dataset\n",
        "        self.names = list(sorted(dataset.keys()))\n",
        "        self.rotate = rotate\n",
        "        self.flip = flip\n",
        "        self.enhance = enhance\n",
        "        self.train = train\n",
        "\n",
        "        self.flip_options = [Image.FLIP_LEFT_RIGHT, Image.FLIP_TOP_BOTTOM]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        if self.train:\n",
        "          index = random.choice(TRAIN_INDEXES)\n",
        "        else:\n",
        "          index = random.choice(TEST_INDEXES)\n",
        "\n",
        "        img_path = self.names[index]\n",
        "        label = self.dataset[img_path]\n",
        "        with Image.open(img_path) as img:\n",
        "\n",
        "            # Apply flip or not\n",
        "            if random.randint(0,1) == 1 and self.flip:\n",
        "              flip_random_ind = random.randint(0,1)\n",
        "              img = img.transpose(self.flip_options[flip_random_ind])\n",
        "\n",
        "            if self.rotate:\n",
        "              rotate_random_angle = random.randint(0,3)\n",
        "              img = img.rotate(rotate_random_angle * 90)\n",
        "\n",
        "            # Apply enhance or not\n",
        "            if random.randint(0,1) == 1 and self.enhance:\n",
        "              factors = np.random.uniform(.5, 1.5, 3)\n",
        "\n",
        "              img = ImageEnhance.Color(img).enhance(factors[0])\n",
        "              img = ImageEnhance.Contrast(img).enhance(factors[1])\n",
        "              img = ImageEnhance.Brightness(img).enhance(factors[2])\n",
        "\n",
        "\n",
        "            img = transforms.ToTensor()(img)\n",
        "\n",
        "            crops = __crop_img__(img.permute(1,2,0).numpy(), 512, 20)\n",
        "            crops = torch.from_numpy(crops).permute(0,3,1,2)\n",
        "            return crops, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "8YUimpSgMhVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhFK22pIY9Hv"
      },
      "outputs": [],
      "source": [
        "# create the dataset\n",
        "dataset_train = ICARDataset(DATASET_PATH, rotate=True, flip=True, enhance=True, train=True)\n",
        "dataset_test = ICARDataset(DATASET_PATH, rotate=True, flip=True, enhance=True, train=False)\n",
        "\n",
        "# create a data loader for train and test sets\n",
        "train_dl = DataLoader(dataset_train, batch_size=25, shuffle=True)\n",
        "test_dl = DataLoader(dataset_test, batch_size=25, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31GtJtrwZCx6"
      },
      "source": [
        "# Feature Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOgDiWlFbqTb"
      },
      "outputs": [],
      "source": [
        "class Identity(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicFeatureExtracter(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BasicFeatureExtracter, self).__init__()\n",
        "        #Resnet50\n",
        "        self.resnet50 = models.resnet50(pretrained=True)\n",
        "        self.resnet50.fc = Identity()\n",
        "        self.resnet50.to(device=DEVICE)\n",
        "        #Inception V3\n",
        "        self.inception_v3 = models.inception_v3(pretrained=True)\n",
        "        self.inception_v3.fc = Identity()\n",
        "        self.inception_v3.to(device=DEVICE)\n",
        "        #VGG16\n",
        "        self.vgg16 = models.vgg16(pretrained=True)\n",
        "        self.vgg16 = self.vgg16.features\n",
        "        self.vgg16.to(device=DEVICE)\n",
        "\n",
        "    def extract_feat_per_crop(self, x):\n",
        "      with torch.no_grad():\n",
        "        res_feat = self.resnet50.forward(x)\n",
        "        inc_v3_feat = self.inception_v3.forward(x)[0]\n",
        "        vgg16_feat = self.vgg16.forward(x)\n",
        "\n",
        "        avg_pool = nn.AvgPool2d(6)\n",
        "        vgg16_feat = avg_pool(vgg16_feat)\n",
        "        vgg16_feat = vgg16_feat.flatten(start_dim=1)\n",
        "\n",
        "      features_concat = torch.cat((res_feat, inc_v3_feat,vgg16_feat), axis=1)\n",
        "      return features_concat\n",
        "\n",
        "\n",
        "    #shape = (Crops=20, x_size=450 ,y_size=450, channels=3)\n",
        "    def forward(self,batch_data):\n",
        "      feature_array = []\n",
        "\n",
        "      for each_image in batch_data:\n",
        "        each_image=each_image.to(device=DEVICE)\n",
        "        features = self.extract_feat_per_crop(each_image)\n",
        "        norm_features = torch.pow(features, 3)\n",
        "        norm_features = torch.sum(norm_features, dim=0)\n",
        "        norm_features = torch.div(norm_features, features.shape[0])\n",
        "        norm_features = torch.pow(norm_features, 1/3)\n",
        "        feature_array.append(norm_features.cpu().numpy())\n",
        "      return np.array(feature_array)\n",
        "\n",
        "basicFeatureExtracter =  BasicFeatureExtracter()"
      ],
      "metadata": {
        "id": "1TfpnVyUMTRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Data"
      ],
      "metadata": {
        "id": "dknEBt-HgUt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run this code to generate data\n",
        "\n",
        "train_dataset_path = f'{PROJECT_PATH}/results_v2/train'\n",
        "test_dataset_path = f'{PROJECT_PATH}/results_v2/test'\n",
        "\n",
        "starting_index = 160\n",
        "\n",
        "# generate train data\n",
        "for i in range(160):\n",
        "  print(f'Epoch: {i+1}')\n",
        "  train_features, train_labels = next(iter(train_dl))\n",
        "  train_labels = train_labels.numpy()\n",
        "  feature_array_train = basicFeatureExtracter.forward(train_features)\n",
        "\n",
        "  feature_csv_name = f'feature_array_{starting_index + i + 1}.csv'\n",
        "  label_csv_name = f'feature_label_{starting_index + i + 1}.csv'\n",
        "  np.savetxt(f'{train_dataset_path}/{feature_csv_name}', feature_array_train, delimiter=\",\")\n",
        "  np.savetxt(f'{train_dataset_path}/{label_csv_name}', train_labels, delimiter=\",\")\n",
        "\n",
        "starting_index = 0\n",
        "# generate test data\n",
        "for i in range(40):\n",
        "  print(f'Epoch: {i+1}')\n",
        "  test_features, test_labels = next(iter(test_dl))\n",
        "  test_labels = test_labels.numpy()\n",
        "  feature_array_train = basicFeatureExtracter.forward(test_features)\n",
        "\n",
        "  feature_csv_name = f'feature_array_{starting_index + i + 1}.csv'\n",
        "  label_csv_name = f'feature_label_{starting_index + i + 1}.csv'\n",
        "  np.savetxt(f'{test_dataset_path}/{feature_csv_name}', feature_array_train, delimiter=\",\")\n",
        "  np.savetxt(f'{test_dataset_path}/{label_csv_name}', test_labels, delimiter=\",\")"
      ],
      "metadata": {
        "id": "hW6S-9xCiUS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data\n"
      ],
      "metadata": {
        "id": "LulMl_DxfAEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path = f'{PROJECT_PATH}/results_v2'\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = None, None, None, None\n",
        "\n",
        "\n",
        "# load train data\n",
        "for i in range(600):\n",
        "  print(f'Feautres for file {i + 1} was loaded')\n",
        "\n",
        "  features_path = f'feature_array_{i+1}.csv'\n",
        "  labels_path = f'feature_label_{i+1}.csv'\n",
        "  features = genfromtxt(f'{drive_path}/train/{features_path}', delimiter=',')\n",
        "  label = genfromtxt(f'{drive_path}/train/{labels_path}', delimiter=',')\n",
        "\n",
        "  if X_train is None:\n",
        "    X_train = features\n",
        "    y_train = label\n",
        "  else:\n",
        "    X_train = np.concatenate([X_train, features])\n",
        "    y_train = np.concatenate([y_train, label])\n",
        "\n",
        "\n",
        "\n",
        "# load test data\n",
        "for i in range(40):\n",
        "  print(f'Feautres for file {i + 1} was loaded')\n",
        "\n",
        "  features_path = f'feature_array_{i+1}.csv'\n",
        "  labels_path = f'feature_label_{i+1}.csv'\n",
        "  features = genfromtxt(f'{drive_path}/test/{features_path}', delimiter=',')\n",
        "  label = genfromtxt(f'{drive_path}/test/{labels_path}', delimiter=',')\n",
        "\n",
        "  if X_test is None:\n",
        "    X_test = features\n",
        "    y_test = label\n",
        "  else:\n",
        "    X_test = np.concatenate([X_test, features])\n",
        "    y_test = np.concatenate([y_test, label])"
      ],
      "metadata": {
        "id": "vpklnBDAzk-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training Dataset Size: {len(X_train)}')\n",
        "print(f'Test Dataset Size: {len(X_test)}')"
      ],
      "metadata": {
        "id": "V_K6P2s94et4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f75aa980-2e12-444d-e68c-4882b0371c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Size: 15000\n",
            "Test Dataset Size: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEv87dmHqh10",
        "outputId": "02682624-b26d-4841-a1d0-8ff09a445745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 6144)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i47Y38kzZGmR"
      },
      "source": [
        "# Training and Testing the Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiCh9ckFiWot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea76e07-fd15-44f0-d60c-0f3728d0b100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss   Remaining Time \n",
            "         1           1.1397          107.10m\n",
            "         2           1.0221          104.47m\n",
            "         3           0.9415          106.14m\n",
            "         4           0.8762          104.14m\n",
            "         5           0.8356          102.05m\n",
            "         6           0.7923          101.27m\n",
            "         7           0.7578           99.69m\n",
            "         8           0.7217           98.28m\n",
            "         9           0.6984           97.00m\n",
            "        10           0.6726           95.70m\n",
            "        20           0.4842           81.10m\n",
            "        30           0.3752           67.17m\n",
            "        40           0.3022           55.83m\n",
            "        50           0.2479           45.65m\n",
            "        60           0.2102           36.11m\n",
            "        70           0.1809           26.85m\n",
            "        80           0.1602           17.77m\n",
            "        90           0.1422            8.84m\n",
            "       100           0.1271            0.00s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.807"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0, verbose=1).fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "tw-3id5f5M0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce19fcc-08f1-4f45-f2cf-6853aa2ea1fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[283  63  21   8]\n",
            " [ 28 160   3   7]\n",
            " [  5  26 222  16]\n",
            " [  0  12   4 142]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = {name: index for index in range(len(LABELS)) for name in glob.glob(DATASET_PATH + '/' + LABELS[index] + '/*.tif')}\n",
        "labels = []\n",
        "feature_array = []\n",
        "for img_inx in TEST_INDEXES:\n",
        "  img_path = list(dataset.keys())[img_inx]\n",
        "  label = dataset[img_path]\n",
        "  labels.append(label)\n",
        "  with Image.open(img_path) as img:\n",
        "      img = transforms.ToTensor()(img)\n",
        "      img = img.to(device=DEVICE)\n",
        "\n",
        "      crops = __crop_img__(img.permute(1,2,0).numpy(), 512, 20)\n",
        "      feature_array.append(crops)\n",
        "labels = np.array(labels)\n",
        "feature_array = torch.from_numpy(np.array(feature_array)).permute(0,1,4,2,3)\n",
        "print(f'Shape of feature array: {feature_array.shape}')\n",
        "print(f'Shape of labels: {labels.shape}')\n",
        "original_image_features = basicFeatureExtracter.forward(feature_array)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRFwNsewf-DJ",
        "outputId": "50a3f702-73ef-4ac8-8e65-a2849bc14ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of feature array: torch.Size([40, 20, 3, 512, 512])\n",
            "Shape of labels: (40,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(original_image_features, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mlH3KFeiiC6",
        "outputId": "b24f6f27-d2f1-4b69-ef44-1d872c144c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.925"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_org = clf.predict(original_image_features)\n",
        "print(confusion_matrix(labels, y_pred_org))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcKDOgLEHJYw",
        "outputId": "b2d53513-0683-46d0-e124-ff2323b56e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 5  2  1  0]\n",
            " [ 0 10  0  0]\n",
            " [ 0  0  7  0]\n",
            " [ 0  0  0 15]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}